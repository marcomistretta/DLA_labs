{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Laboratory\n",
    "\n",
    "In this laboratory session we will hack one of your colleague's (Francesco Fantechi, from Ingegneria Informatica) implementation of a navigation environment for Deep Reinforcement Learning. The setup is fairly simple:\n",
    "\n",
    "+ A simple 2D environment with a (limited) number of *obstacles* and a single *goal* is presented to the agent, which must learn how to navigate to the goal without hitting any obstacles.\n",
    "+ The agent *observes* the environment via a set of 16 rays cast uniformly which return the distance to the first obstacle encountered, as well as the distance and direction to the goal.\n",
    "+ The agent has three possible actions: `ROTATE LEFT`, `ROTATE RIGHT`, or `MOVE FORWARD`.\n",
    "\n",
    "For each step of an episode, the agent receives a reward of:\n",
    "+ -100 if hitting an obstacle (episode ends).\n",
    "+ -100 if one hundred steps are reached without hitting the goal.\n",
    "+ +100 if hitting the goal (episode ends)\n",
    "+ A small *positive* reward if the distance to the goal is *reduced*.\n",
    "+ A small *negative* reward if the distance to the goal is *increased*.\n",
    "\n",
    "In the file `main.py` you will find an implementation of **Deep Q-Learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
   "metadata": {},
   "source": [
    "## Exercise 1: Testing the Environment\n",
    "\n",
    "The first thing to do is verify that the environment is working in your Anaconda virtual environment. I had a weird problem with Tensorboard and had to downgrade it using:\n",
    "\n",
    "    conda install -c conda-forge tensorboard=2.11.2\n",
    "    \n",
    "In any case, you should be able to run:\n",
    "\n",
    "    python main.py\n",
    "    \n",
    "from the repository root and it will run episodes using a pretrained agent. To train an agent from scratch, you must modify `main.py` setting `TRAIN = True` at the top. Then running `main.py` again will train an agent for 2000 episodes of training. To run the trained agent you will again have to modify `main.py` on line 225 to load the last saved checkpoint:\n",
    "\n",
    "    PATH = './checkpoints/last.pth'\n",
    "    \n",
    "and then run the script again (after setting `TRAIN = False` !).\n",
    "\n",
    "Make sure you can at run the demo agent and train one from scratch. If you don't have a GPU you can set the number of training episodes to a smaller number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b8cf76-5d39-45e9-8d7d-324125c04b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env_render = gym.make('gym_navigation:NavigationGoal-v0', render_mode=None, track_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd2dd9d-a60b-4892-878f-d83f64ac5f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(obs, info) = env_render.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8422e70-f95b-4703-b9ab-7df80203b34c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.25013485, 4.59240568, 4.62203256, 1.59736348, 8.37985615,\n",
       "        2.11246148, 7.39143428, 3.5392273 , 2.93893401, 2.8238255 ,\n",
       "        3.3256795 , 4.77303783, 5.66876574, 5.5521309 , 6.38526811,\n",
       "        8.97439628, 6.3003271 , 1.00109247]),\n",
       " {'result': 'Failed'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2b4d07-1099-466f-b585-fe96b54705de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs2 = env_render.step(env_render.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909d1442-85a3-40bd-b7ea-7256f18e8292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, [15.         15.         15.         15.         15.         15.\n",
       " 15.         15.         15.         15.         15.         15.\n",
       " 15.         15.         15.         15.         15.          3.14159265], (18,), float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_render.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed9279-3a2f-4fcd-bf29-b105b2da8433",
   "metadata": {},
   "source": [
    "## Exercise 2: Stabilizing Q-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
   "metadata": {},
   "source": [
    "## Exercise 3: Going Deeper\n",
    "\n",
    "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Solving the environment with `REINFORCE`\n",
    "\n",
    "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the environment.\n",
    "\n",
    "**Note**: There is a *design flaw* in the environment implementation that will lead to strange (by explainable) behavior in agents trained with `REINFORCE`. See if you can figure it out and fix it.\n",
    "\n",
    "### Exercise 3.2: Solving another environment\n",
    "\n",
    "The [Gymnasium](https://gymnasium.farama.org/) framework has a ton of interesting and fun environments to work with. Pick one and try to solve it using any technique you like. The [Lunar Landar](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment is a fun one. \n",
    "\n",
    "### Exercise 3.3: Advanced techniques \n",
    "\n",
    "The `REINFORCE` and Q-Learning approaches, though venerable, are not even close to the state-of-the-art. Try using an off-the-shelf implementation of [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347) to solve one (or more) of these environments. Compare your results with those of Q-Learning and/or REINFORCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b8a52-d97b-4c32-bfcb-9069e7527ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
